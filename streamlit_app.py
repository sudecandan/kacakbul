import streamlit as st
import pandas as pd
import zipfile
from io import BytesIO

# STREAMLIT BAÅLIÄI
st.title("âš¡ KaÃ§akBul")

# KullanÄ±cÄ±dan dosya yÃ¼kleme iÃ§in iki sÃ¼tun
col1, col2 = st.columns(2)

with col1:
    el31_file = st.file_uploader("ğŸ“‚ EL31 DosyasÄ±nÄ± YÃ¼kleyin (.csv)", type=["csv"])
    
with col2:
    zblir_file = st.file_uploader("ğŸ“‚ ZBLIR_002 DosyasÄ±nÄ± YÃ¼kleyin (.csv)", type=["csv"])

# KullanÄ±cÄ± dosyalarÄ± yÃ¼klediyse Ã¶nizleme gÃ¶ster
if el31_file and zblir_file:
    st.subheader("ğŸ“Š YÃ¼klenen Dosya Ã–nizlemesi")
    
    col1, col2 = st.columns(2)

    with col1:
        df_el31 = pd.read_csv(el31_file, delimiter=";", encoding="utf-8")
        st.write("ğŸ”¹ **EL31 DosyasÄ± Ã–nizleme**")
        st.dataframe(df_el31.head())

    with col2:
        df_zblir = pd.read_csv(zblir_file, delimiter=";", encoding="utf-8")
        st.write("ğŸ”¹ **ZBLIR_002 DosyasÄ± Ã–nizleme**")
        st.dataframe(df_zblir.head())

# **EL31 VERÄ°LERÄ°NÄ° DÃœZENLE BUTONU**
if el31_file and st.button("ğŸ“Œ EL31 Verilerini DÃ¼zenle"):

    def clean_el31(df):
        drop_columns = [
            "SÃ¶zleÅŸme grubu", "SayaÃ§ okuma birimi", "Muhatap", "SÃ¶zleÅŸme", "Cihaz", "Ekipman", "Endeks",
            "GiriÅŸ numarasÄ±", "Kontrol rakamÄ±", "Planlanan SO tarihi", "SayaÃ§ okuma nedeni", "Ã‡oklu tayin",
            "Pln.sayaÃ§ okuma tipi", "SayaÃ§ okuma tÃ¼rÃ¼", "SayaÃ§ okuma durumu", "Vrg.Ã¶nc.basamaklar", "OndalÄ±k basamaklar",
            "Hizmet sipariÅŸi", "Hizmet bildirimi", "SO belge dahili tn.", "SprÅŸ.Ã§kt.Ã¶nc.alÄ±ndÄ±", "BaÄŸÄ±msÄ±z doÄŸrulama",
            "BaÄŸlÄ± doÄŸrulama", "SayaÃ§ notu", "Geriye dÃ¶nÃ¼k thk.drm.", "SayaÃ§ okuma etkin", "GeliÅŸmiÅŸ sayaÃ§ okuma sistemi",
            "Ä°letim durumu kodu", "Zaman damgasÄ±", "Kaynak sistem.1", "Aktarma tarihi", "AktarÄ±m saati",
            "Ä°letim durumu", "Ä°letim durumu tanÄ±mÄ±", "Kaynak sistem", "DoÄŸal sayÄ±", "FarklÄ± sÃ¶zleÅŸme gr.",
            "Tahakkuk edilecek sayaÃ§ durumu", "Katalog 1", "Kod grubu 1", "Kod 1", "AÃ§Ä±klama 1", "Bildirim 1",
            "Katalog 2", "Kod grubu 2", "Kod 2", "AÃ§Ä±klama 2", "Bildirim 2", "Katalog 3", "Kod grubu 3",
            "Kod 3", "AÃ§Ä±klama 3", "Bildirim 3", "Deneme SayÄ±sÄ±", "Okuma ZamanÄ±", "Manually-read"
        ]
        return df.drop(columns=drop_columns, errors='ignore')

    def only_p_lines(df):
        return df[df["Endeks tÃ¼rÃ¼"] == "P"]

    def filter_max_reading(df):
        df["Okunan sayaÃ§ durumu"] = df["Okunan sayaÃ§ durumu"].astype(str).str.replace(",", ".").astype(float)
        df = df.sort_values(by=["Tesisat", "SayaÃ§ okuma tarihi", "Okunan sayaÃ§ durumu"], ascending=[True, True, False])
        return df.groupby(["Tesisat", "SayaÃ§ okuma tarihi"], as_index=False).first()

    # **EL31 Verilerini Temizleme**
    df_el31_cleaned = clean_el31(df_el31)
    df_el31_cleaned = only_p_lines(df_el31_cleaned)
    df_el31_filtered = filter_max_reading(df_el31_cleaned)

    # **ZIP dosyasÄ±na kaydetme**
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zipf:
        for tesisat, group in df_el31_filtered.groupby("Tesisat"):
            unique_muhatap = group["Muhatap adÄ±"].unique()

            if len(unique_muhatap) == 1:
                file_name = f"{tesisat}.csv"
                csv_data = group.to_csv(sep=";", index=False).encode("utf-8")
                zipf.writestr(file_name, csv_data)

            elif len(unique_muhatap) == 2:
                latest_muhatap = unique_muhatap[0]
                file_name_A = f"{tesisat}-A.csv"
                csv_data_A = group[group["Muhatap adÄ±"] == latest_muhatap].to_csv(sep=";", index=False).encode("utf-8")
                zipf.writestr(file_name_A, csv_data_A)

                file_name_AB = f"{tesisat}-AB.csv"
                csv_data_AB = group.to_csv(sep=";", index=False).encode("utf-8")
                zipf.writestr(file_name_AB, csv_data_AB)

    zip_buffer.seek(0)

    st.success("âœ… EL31 Verileri DÃ¼zenlendi!")
    st.download_button("ğŸ“¥ DÃ¼zenlenmiÅŸ EL31 DosyalarÄ±nÄ± ZIP Olarak Ä°ndir", zip_buffer, "el31_duzenlenmis.zip", "application/zip")




# **ZBLIR_002 VERÄ°LERÄ°NÄ° DÃœZENLE BUTONU**
if zblir_file and st.button("ğŸ“Œ ZBLIR_002 Verilerini DÃ¼zenle"):
    def filter_latest_two_contacts(df):
        """Her tesisat iÃ§in en gÃ¼ncel iki muhatabÄ± seÃ§er."""
        df["Son Okuma Tarihi"] = pd.to_datetime(df["Son Okuma Tarihi"], dayfirst=True)
        df = df.sort_values(by=["Tesisat", "Son Okuma Tarihi"], ascending=[True, False])
        df = df.groupby("Tesisat").apply(lambda x: x[x["Muhatap AdÄ±"].isin(x["Muhatap AdÄ±"].unique()[:2])])
        return df.reset_index(drop=True)

    df_zblir_cleaned = filter_latest_two_contacts(df_zblir)

    # **ZIP DOSYASI OLUÅTURMA**
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zipf:
        for tesisat, group in df_zblir_cleaned.groupby("Tesisat"):
            unique_muhatap = group["Muhatap AdÄ±"].unique()

            if len(unique_muhatap) == 1:
                file_name = f"{tesisat}.csv"
                csv_data = group.to_csv(sep=";", index=False).encode("utf-8")
                zipf.writestr(file_name, csv_data)

            elif len(unique_muhatap) == 2:
                latest_muhatap = unique_muhatap[0]

                file_name_A = f"{tesisat}-A.csv"
                csv_data_A = group[group["Muhatap AdÄ±"] == latest_muhatap].to_csv(sep=";", index=False).encode("utf-8")
                zipf.writestr(file_name_A, csv_data_A)

                file_name_AB = f"{tesisat}-AB.csv"
                csv_data_AB = group.to_csv(sep=";", index=False).encode("utf-8")
                zipf.writestr(file_name_AB, csv_data_AB)

    zip_buffer.seek(0)

    st.success("âœ… ZBLIR_002 Verileri DÃ¼zenlendi!")
    st.download_button("ğŸ“¥ DÃ¼zenlenmiÅŸ ZBLIR_002 DosyalarÄ±nÄ± ZIP Olarak Ä°ndir", zip_buffer, "zblir_duzenlenmis.zip", "application/zip")







#BURAYA KADAR OKEYYYYYYYYYY



# ğŸ“Š KullanÄ±cÄ±dan analiz iÃ§in giriÅŸ al
col1, col2 = st.columns([1, 1])  

# ğŸŸ¢ **Analiz SeÃ§enekleri**
with col1:
    st.markdown("#### ğŸ“Š Hangi Analiz YapÄ±lacak?")

    # SeÃ§eneklerin listesi
    analysis_options = ["P Analizi", "T1 Analizi", "T2 Analizi", "T3 Analizi"]

    # Session state iÃ§inde checkbox durumlarÄ±nÄ± sakla
    if "selected_analysis" not in st.session_state:
        st.session_state.selected_analysis = {opt: False for opt in analysis_options}

    # CheckboxlarÄ± oluÅŸtur
    for option in analysis_options:
        st.session_state.selected_analysis[option] = st.checkbox(option, st.session_state.selected_analysis[option])

    # TÃ¼mÃ¼nÃ¼ SeÃ§ butonu
    def toggle_all():
        all_selected = all(st.session_state.selected_analysis.values())
        for key in st.session_state.selected_analysis:
            st.session_state.selected_analysis[key] = not all_selected  # Tersine Ã§evir

    st.button("TÃ¼mÃ¼nÃ¼ SeÃ§", on_click=toggle_all)

# ğŸ”µ **DÃ¼ÅŸÃ¼ÅŸ Parametreleri**
with col2:
    st.markdown("#### ğŸ“‰ DÃ¼ÅŸÃ¼ÅŸ Parametreleri")
    decrease_percentage = st.number_input("ğŸ“‰ YÃ¼zde KaÃ§ DÃ¼ÅŸÃ¼ÅŸ?", min_value=1, max_value=100, step=1, value=30)
    decrease_count = st.number_input("ğŸ”„ KaÃ§ Kez DÃ¼ÅŸÃ¼ÅŸ?", min_value=1, max_value=10, step=1, value=3)

# **SeÃ§ili analizleri belirleme**
selected_analysis = [key for key, value in st.session_state.selected_analysis.items() if value]












#BURAYA KADAR DA OKEYYYY GÄ°BÄ°


# ğŸš€ **P Analizi Fonksiyonu**
def p_analizi(extracted_files, esik_orani, alt_esik_sayisi):
    suspicious_tesisats = []

    for file_name, file_data in extracted_files.items():
        df = pd.read_csv(BytesIO(file_data), delimiter=';', encoding='utf-8')

        # **SayÄ±sal verileri doÄŸru ÅŸekilde parse et**
        df['Okunan sayaÃ§ durumu'] = df['Okunan sayaÃ§ durumu'].astype(str).str.replace(',', '.').astype(float)

        for tesisat, group in df.groupby('Tesisat'):
            p_values = group[group['Endeks tÃ¼rÃ¼'] == 'P']['Okunan sayaÃ§ durumu'].dropna().tolist()

            if not p_values:
                continue  # EÄŸer P deÄŸeri yoksa atla

            p_values_nonzero = [val for val in p_values if val > 0]

            if len(p_values_nonzero) > 0:
                p_avg = sum(p_values_nonzero) / len(p_values_nonzero)
                esik_deger = p_avg * (1 - esik_orani)

                below_threshold_count = sum(1 for val in p_values_nonzero if val < esik_deger)

                # Son Ã¼Ã§ P deÄŸerinin ortalamanÄ±n Ã¼stÃ¼nde olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                last_three_values = p_values_nonzero[-3:] if len(p_values_nonzero) >= 3 else []
                if all(val > p_avg for val in last_three_values):
                    continue  # Son Ã¼Ã§ deÄŸer ortalamanÄ±n Ã¼stÃ¼ndeyse ÅŸÃ¼pheli listeye ekleme

                if below_threshold_count > alt_esik_sayisi:
                    suspicious_tesisats.append([tesisat])

    # ÅÃ¼pheli tesisatlarÄ± yeni bir CSV dosyasÄ±na yazdÄ±r
    suspicious_df = pd.DataFrame(suspicious_tesisats, columns=['ÅÃ¼pheli Tesisat'])
    return suspicious_df

# ğŸš€ **Analizi BaÅŸlat Butonu**
if st.button("ğŸš€ Analizi BaÅŸlat"):
    if "P Analizi" in selected_analysis and uploaded_zip:
        with zipfile.ZipFile(uploaded_zip, 'r') as zip_ref:
            extracted_files = {name: zip_ref.read(name) for name in zip_ref.namelist() if name.endswith(".csv")}

        st.success(f"âœ… {len(extracted_files)} dosya baÅŸarÄ±yla aÃ§Ä±ldÄ±!")

        # **P Analizini Ã‡alÄ±ÅŸtÄ±r**
        df_suspicious_p = p_analizi(extracted_files, decrease_percentage, decrease_count)

        if not df_suspicious_p.empty:
            st.success("âœ… P Analizi TamamlandÄ±!")
            st.dataframe(df_suspicious_p)

            # **Ä°ndirme iÃ§in CSV DosyasÄ± HazÄ±rla**
            csv_buffer = BytesIO()
            df_suspicious_p.to_csv(csv_buffer, sep=";", index=False, encoding="utf-8")
            csv_buffer.seek(0)

            st.download_button(
                "ğŸ“¥ P Analizi SonuÃ§larÄ±nÄ± Ä°ndir",
                csv_buffer,
                "p_analizi_sonuclar.csv",
                "text/csv"
            )
        else:
            st.warning("âŒ ÅÃ¼pheli tesisat bulunamadÄ±!")














